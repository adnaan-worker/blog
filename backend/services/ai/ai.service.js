const aiModel = require('./core/ai-model.service');
const streamManager = require('./core/stream-manager');
const promptManager = require('./prompts');
const { StringOutputParser } = require('@langchain/core/output_parsers');
const { logger } = require('@/utils/logger');

/**
 * AI æœåŠ¡ - ç»Ÿä¸€çš„ AI è°ƒç”¨æ¥å£
 * åŸºäº LangChain.js çš„ç°ä»£åŒ–å°è£…
 */
class AIService {
  constructor() {
    this.initialized = false;
  }

  /**
   * åˆå§‹åŒ–æœåŠ¡
   */
  async initialize() {
    if (this.initialized) {
      return true;
    }

    try {
      await aiModel.initialize();
      this.initialized = true;
      return true;
    } catch (error) {
      throw error;
    }
  }

  /**
   * ç¡®ä¿æœåŠ¡å·²åˆå§‹åŒ–
   */
  _ensureInitialized() {
    if (!this.initialized) {
      throw new Error('AI æœåŠ¡æœªåˆå§‹åŒ–ï¼Œè¯·å…ˆè°ƒç”¨ initialize()');
    }
  }

  /**
   * ç®€å•èŠå¤©ï¼ˆéæµå¼ï¼‰
   * @param {string} message - ç”¨æˆ·æ¶ˆæ¯
   * @param {Object} options - é…ç½®é€‰é¡¹
   */
  async chat(message, options = {}) {
    this._ensureInitialized();

    const { systemPrompt = null } = options;
    const modelInfo = aiModel.getCurrentModel();

    logger.info('ğŸ¤– è°ƒç”¨ LLM', {
      provider: modelInfo.provider,
      model: modelInfo.model,
      type: 'chat',
      messageLength: message.length,
    });

    const messages = [];

    if (systemPrompt) {
      messages.push({ role: 'system', content: systemPrompt });
    }

    messages.push({ role: 'user', content: message });

    const model = aiModel.getModel();
    const response = await model.invoke(messages);

    logger.info('âœ… LLM å“åº”å®Œæˆ', {
      provider: modelInfo.provider,
      model: modelInfo.model,
      responseLength: response.content.length,
    });

    return response.content;
  }

  /**
   * æµå¼èŠå¤©
   * @param {string} message - ç”¨æˆ·æ¶ˆæ¯
   * @param {Function} onChunk - chunk å›è°ƒ
   * @param {Object} options - é…ç½®é€‰é¡¹
   */
  async streamChat(message, onChunk, options = {}) {
    this._ensureInitialized();

    const { systemPrompt = null, taskId = `chat_${Date.now()}` } = options;
    const modelInfo = aiModel.getCurrentModel();

    logger.info('ğŸ¤– è°ƒç”¨ LLM (æµå¼)', {
      provider: modelInfo.provider,
      model: modelInfo.model,
      type: 'stream_chat',
      taskId,
      messageLength: message.length,
    });

    const messages = [];

    if (systemPrompt) {
      messages.push({ role: 'system', content: systemPrompt });
    }

    messages.push({ role: 'user', content: message });

    const streamingModel = aiModel.getStreamingModel(); // ä½¿ç”¨æµå¼æ¨¡å‹
    const stream = await streamingModel.stream(messages);

    const controller = streamManager.createStream(taskId, stream, options);

    const result = await controller.start(onChunk);

    logger.info('âœ… LLM æµå¼å“åº”å®Œæˆ', {
      provider: modelInfo.provider,
      model: modelInfo.model,
      taskId,
      contentLength: result.length,
    });

    return result;
  }

  /**
   * ä½¿ç”¨æ¨¡æ¿ç”Ÿæˆå†…å®¹ï¼ˆéæµå¼ï¼‰
   * @param {string} templateName - æ¨¡æ¿åç§°
   * @param {Object} variables - æ¨¡æ¿å˜é‡
   */
  async generate(templateName, variables) {
    this._ensureInitialized();

    const modelInfo = aiModel.getCurrentModel();
    const template = promptManager.getTemplate(templateName);
    const model = aiModel.getModel();
    const chain = template.pipe(model).pipe(new StringOutputParser());

    logger.info('ğŸ¤– è°ƒç”¨ LLM', {
      provider: modelInfo.provider,
      model: modelInfo.model,
      template: templateName,
    });

    try {
      const response = await Promise.race([
        chain.invoke(variables),
        new Promise((_, reject) =>
          setTimeout(() => reject(new Error('LLM è°ƒç”¨è¶…æ—¶ï¼ˆ180ç§’ï¼‰')), 180000)
        ),
      ]);

      logger.info('âœ… LLM ç”Ÿæˆå®Œæˆ', {
        provider: modelInfo.provider,
        model: modelInfo.model,
        template: templateName,
      });

      return response;
    } catch (error) {
      logger.error('âŒ LLM è°ƒç”¨å¤±è´¥', {
        provider: modelInfo.provider,
        model: modelInfo.model,
        template: templateName,
        error: error.message,
      });
      throw error;
    }
  }

  /**
   * ä½¿ç”¨æ¨¡æ¿ç”Ÿæˆå†…å®¹ï¼ˆæµå¼ï¼‰
   * @param {string} templateName - æ¨¡æ¿åç§°
   * @param {Object} variables - æ¨¡æ¿å˜é‡
   * @param {Function} onChunk - chunk å›è°ƒ
   * @param {Object} options - é…ç½®é€‰é¡¹
   */
  async streamGenerate(templateName, variables, onChunk, options = {}) {
    this._ensureInitialized();

    const { taskId = `${templateName}_${Date.now()}` } = options;
    const modelInfo = aiModel.getCurrentModel();

    logger.info('ğŸ¤– è°ƒç”¨ LLM (æµå¼æ¨¡æ¿)', {
      provider: modelInfo.provider,
      model: modelInfo.model,
      type: 'stream_generate',
      template: templateName,
      taskId,
    });

    const template = promptManager.getTemplate(templateName);
    const streamingModel = aiModel.getStreamingModel(); // ä½¿ç”¨æµå¼æ¨¡å‹
    const chain = template.pipe(streamingModel).pipe(new StringOutputParser());

    const stream = await chain.stream(variables);

    const controller = streamManager.createStream(taskId, stream, options);

    const result = await controller.start(onChunk);

    logger.info('âœ… LLM æµå¼æ¨¡æ¿ç”Ÿæˆå®Œæˆ', {
      provider: modelInfo.provider,
      model: modelInfo.model,
      template: templateName,
      taskId,
      contentLength: result.length,
    });

    return result;
  }

  /**
   * å–æ¶ˆæµå¼ä»»åŠ¡
   * @param {string} taskId - ä»»åŠ¡ID
   */
  cancelStream(taskId) {
    return streamManager.cancelStream(taskId);
  }

  /**
   * è·å–æµå¼ä»»åŠ¡çŠ¶æ€
   * @param {string} taskId - ä»»åŠ¡ID
   */
  getStreamStatus(taskId) {
    const controller = streamManager.getStream(taskId);
    return controller ? controller.getStatus() : null;
  }

  /**
   * è·å–æœåŠ¡ä¿¡æ¯
   */
  getInfo() {
    return {
      ...aiModel.getInfo(),
      activeStreams: streamManager.getActiveCount(),
      availableTemplates: promptManager.getAvailableTemplates(),
    };
  }

  /**
   * æ£€æŸ¥æœåŠ¡æ˜¯å¦å¯ç”¨
   */
  isAvailable() {
    return this.initialized && aiModel.isAvailable();
  }
}

module.exports = new AIService();
