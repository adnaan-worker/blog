const aiModel = require('./core/ai-model.service');
const streamManager = require('./core/stream-manager');
const promptManager = require('./prompts');
const { logger } = require('@/utils/logger');
const { tools: defaultTools } = require('./tools');
let ToolMessage; // åŠ¨æ€å¯¼å…¥

let StringOutputParser;

const getStringOutputParser = async () => {
  if (!StringOutputParser) {
    const mod = await import('@langchain/core/output_parsers');
    StringOutputParser = mod.StringOutputParser;
  }
  return StringOutputParser;
};

const getToolMessageClass = async () => {
  if (!ToolMessage) {
    const mod = await import('@langchain/core/messages');
    ToolMessage = mod.ToolMessage;
  }
  return ToolMessage;
};

/**
 * AI æœåŠ¡ - ç»Ÿä¸€çš„ AI è°ƒç”¨æ¥å£
 * åŸºäº LangChain.js çš„ç°ä»£åŒ–å°è£…
 */
class AIService {
  constructor() {
    this.initialized = false;
  }

  /**
   * åˆå§‹åŒ–æœåŠ¡
   */
  async initialize() {
    if (this.initialized) {
      return true;
    }

    try {
      await aiModel.initialize();
      this.initialized = true;
      return true;
    } catch (error) {
      throw error;
    }
  }

  /**
   * ç¡®ä¿æœåŠ¡å·²åˆå§‹åŒ–
   */
  _ensureInitialized() {
    if (!this.initialized) {
      throw new Error('AI æœåŠ¡æœªåˆå§‹åŒ–ï¼Œè¯·å…ˆè°ƒç”¨ initialize()');
    }
  }

  /**
   * ç®€å•èŠå¤©ï¼ˆæ”¯æŒå·¥å…·è°ƒç”¨ï¼‰
   * @param {string} message - ç”¨æˆ·æ¶ˆæ¯
   * @param {Object} options - é…ç½®é€‰é¡¹
   */
  async chat(message, options = {}) {
    this._ensureInitialized();

    const { systemPrompt = null, enableTools = false } = options;
    const modelInfo = aiModel.getCurrentModel();

    logger.info('ğŸ¤– è°ƒç”¨ LLM', {
      provider: modelInfo.provider,
      model: modelInfo.model,
      type: 'chat',
      enableTools,
      messageLength: message.length,
    });

    const messages = [];

    if (systemPrompt) {
      messages.push({ role: 'system', content: systemPrompt });
    }

    messages.push({ role: 'user', content: message });

    let model = aiModel.getModel();
    const toolsMap = {};

    // ç»‘å®šå·¥å…·
    if (enableTools) {
      model = model.bindTools(defaultTools);
      defaultTools.forEach(t => (toolsMap[t.name] = t));
    }

    // å·¥å…·è°ƒç”¨å¾ªç¯
    const ToolMessageClass = await getToolMessageClass();
    let finalResponse;
    let turn = 0;
    const maxTurns = 5; // é˜²æ­¢æ­»å¾ªç¯

    while (turn < maxTurns) {
      const response = await model.invoke(messages);
      messages.push(response); // å°† AI å›å¤åŠ å…¥å†å²

      // æ£€æŸ¥æ˜¯å¦æœ‰å·¥å…·è°ƒç”¨
      if (response.tool_calls && response.tool_calls.length > 0) {
        logger.info(`ğŸ› ï¸ AI è¯·æ±‚è°ƒç”¨å·¥å…·: ${response.tool_calls.length} ä¸ª`, {
          tools: response.tool_calls.map(t => t.name),
        });

        // å¹¶è¡Œæ‰§è¡Œæ‰€æœ‰å·¥å…·è°ƒç”¨
        await Promise.all(
          response.tool_calls.map(async toolCall => {
            const tool = toolsMap[toolCall.name];
            if (tool) {
              try {
                logger.info(`ğŸ”§ æ‰§è¡Œå·¥å…·: ${toolCall.name}`, toolCall.args);
                const result = await tool.invoke(toolCall.args);

                messages.push(
                  new ToolMessageClass({
                    tool_call_id: toolCall.id,
                    content: result,
                    name: toolCall.name,
                  })
                );
              } catch (e) {
                logger.error(`âŒ å·¥å…·æ‰§è¡Œå¤±è´¥: ${toolCall.name}`, e);
                messages.push(
                  new ToolMessageClass({
                    tool_call_id: toolCall.id,
                    content: `Error executing tool: ${e.message}`,
                    name: toolCall.name,
                  })
                );
              }
            }
          })
        );

        turn++;
      } else {
        // æ²¡æœ‰å·¥å…·è°ƒç”¨ï¼Œç›´æ¥è¿”å›ç»“æœ
        finalResponse = response;
        break;
      }
    }

    if (!finalResponse) {
      return 'æŠ±æ­‰ï¼Œæˆ‘å¤„ç†è¿™ä¸ªè¯·æ±‚æ—¶é‡åˆ°äº†ä¸€äº›å›°éš¾ï¼ˆå·¥å…·è°ƒç”¨å¾ªç¯æ¬¡æ•°è¶…é™ï¼‰ã€‚';
    }

    logger.info('âœ… LLM å“åº”å®Œæˆ', {
      provider: modelInfo.provider,
      model: modelInfo.model,
      responseLength: finalResponse.content.length,
      turns: turn,
    });

    return finalResponse.content;
  }

  /**
   * æµå¼èŠå¤©ï¼ˆæ”¯æŒå·¥å…·è°ƒç”¨ï¼‰
   * @param {string} message - ç”¨æˆ·æ¶ˆæ¯
   * @param {Function} onChunk - chunk å›è°ƒ
   * @param {Object} options - é…ç½®é€‰é¡¹
   */
  async streamChat(message, onChunk, options = {}) {
    this._ensureInitialized();

    const {
      systemPrompt = null,
      taskId = `chat_${Date.now()}`,
      messages: customMessages,
      enableTools = true, // æ˜¯å¦å¯ç”¨å·¥å…·
    } = options;

    const modelInfo = aiModel.getCurrentModel();

    logger.info('ğŸ¤– è°ƒç”¨ LLM (æµå¼)', {
      provider: modelInfo.provider,
      model: modelInfo.model,
      type: 'stream_chat',
      taskId,
      enableTools,
      messageLength: message.length,
      hasHistory: !!customMessages,
    });

    // æ„å»ºæ¶ˆæ¯å†å²
    let messages;
    if (customMessages && Array.isArray(customMessages)) {
      messages = customMessages;
    } else {
      messages = [];
      if (systemPrompt) {
        messages.push({ role: 'system', content: systemPrompt });
      }
      messages.push({ role: 'user', content: message });
    }

    let model = aiModel.getStreamingModel();
    const toolsMap = {};

    // ç»‘å®šå·¥å…·
    if (enableTools) {
      model = model.bindTools(defaultTools);
      defaultTools.forEach(t => (toolsMap[t.name] = t));
    }

    // ä½¿ç”¨ StreamManager åˆ›å»ºæ§åˆ¶å™¨ï¼ˆç”¨äºçŠ¶æ€ç®¡ç†å’Œå–æ¶ˆï¼‰
    // è¿™é‡Œæˆ‘ä»¬éœ€è¦è‡ªå®šä¹‰æµçš„æ‰§è¡Œé€»è¾‘ï¼Œæ‰€ä»¥ä¼ å…¥ä¸€ä¸ªç©ºçš„ç”Ÿæˆå™¨å ä½ï¼Œåç»­æ‰‹åŠ¨æ§åˆ¶
    async function* emptyGenerator() {}
    const controller = streamManager.createStream(taskId, emptyGenerator(), options);

    // æ‰‹åŠ¨è®¾ç½®çŠ¶æ€ä¸º running
    controller.status = 'running';
    controller.startTime = Date.now();
    controller.emit('start', { taskId });

    const ToolMessageClass = await getToolMessageClass();
    let turn = 0;
    const maxTurns = 5;
    let fullContent = '';

    try {
      while (turn < maxTurns) {
        // æ£€æŸ¥å–æ¶ˆçŠ¶æ€
        if (controller.isCancelled) {
          controller.emit('cancelled', { taskId });
          return fullContent;
        }

        // å¼€å§‹æµå¼ç”Ÿæˆ
        const stream = await model.stream(messages);

        let aggregatedChunk = null;

        // æ¶ˆè´¹æµ
        for await (const chunk of stream) {
          if (controller.isCancelled) break;

          // ç´¯ç§¯ chunk (LangChain è‡ªåŠ¨å¤„ç†åˆå¹¶)
          aggregatedChunk = aggregatedChunk ? aggregatedChunk.concat(chunk) : chunk;

          // 1. å¤„ç†æ–‡æœ¬å†…å®¹å¹¶å®æ—¶æ¨é€
          const content = chunk.content;
          if (content && typeof content === 'string' && content.length > 0) {
            fullContent += content;

            if (onChunk) {
              await onChunk(content, {
                taskId,
                totalLength: fullContent.length,
                chunkCount: 0,
              });
            }
          }
        }

        if (controller.isCancelled) break;
        if (!aggregatedChunk) break; // æ²¡æœ‰ä»»ä½•è¾“å‡º

        // å°†è¿™ä¸€è½®çš„å®Œæ•´å›å¤åŠ å…¥å†å²
        messages.push(aggregatedChunk);

        // æ£€æŸ¥æ˜¯å¦æœ‰å·¥å…·è°ƒç”¨
        if (aggregatedChunk.tool_calls && aggregatedChunk.tool_calls.length > 0) {
          logger.info(`ğŸ› ï¸ AI è¯·æ±‚è°ƒç”¨å·¥å…· (æµå¼): ${aggregatedChunk.tool_calls.length} ä¸ª`);

          // æç¤ºå‰ç«¯æ­£åœ¨è°ƒç”¨å·¥å…·ï¼ˆå¯é€‰ï¼Œå¯ä»¥é€šè¿‡å‘é€ç‰¹æ®Šæ–‡æœ¬æˆ–äº‹ä»¶ï¼Œè¿™é‡Œç®€å•å¤„ç†ï¼‰
          // if (onChunk) await onChunk('\n*(æ­£åœ¨æœç´¢ä¿¡æ¯...)*\n', ...);

          // å¹¶è¡Œæ‰§è¡Œå·¥å…·
          await Promise.all(
            aggregatedChunk.tool_calls.map(async toolCall => {
              const tool = toolsMap[toolCall.name];
              if (tool) {
                try {
                  logger.info(`ğŸ”§ æ‰§è¡Œå·¥å…·: ${toolCall.name}`, toolCall.args);
                  const result = await tool.invoke(toolCall.args);

                  messages.push(
                    new ToolMessageClass({
                      tool_call_id: toolCall.id,
                      content: result,
                      name: toolCall.name,
                    })
                  );
                } catch (e) {
                  logger.error(`âŒ å·¥å…·æ‰§è¡Œå¤±è´¥: ${toolCall.name}`, e);
                  messages.push(
                    new ToolMessageClass({
                      tool_call_id: toolCall.id,
                      content: `Error: ${e.message}`,
                      name: toolCall.name,
                    })
                  );
                }
              }
            })
          );

          // å·¥å…·æ‰§è¡Œå®Œï¼Œè¿›å…¥ä¸‹ä¸€è½®å¾ªç¯ï¼ˆLLM ä¼šçœ‹åˆ°å·¥å…·ç»“æœå¹¶ç”Ÿæˆå›ç­”ï¼‰
          turn++;
        } else {
          // æ²¡æœ‰å·¥å…·è°ƒç”¨ï¼Œè¯´æ˜æ˜¯æœ€ç»ˆå›å¤ï¼Œç»“æŸå¾ªç¯
          break;
        }
      }

      // å®Œæˆ
      controller.status = 'done';
      controller.endTime = Date.now();
      controller.emit('done', { taskId, content: fullContent });
    } catch (error) {
      controller.status = 'error';
      controller.emit('error', { taskId, error: error.message });
      throw error;
    }

    return fullContent;
  }

  /**
   * ä½¿ç”¨æ¨¡æ¿ç”Ÿæˆå†…å®¹ï¼ˆéæµå¼ï¼‰
   * @param {string} templateName - æ¨¡æ¿åç§°
   * @param {Object} variables - æ¨¡æ¿å˜é‡
   */
  async generate(templateName, variables) {
    this._ensureInitialized();

    const modelInfo = aiModel.getCurrentModel();
    const template = promptManager.getTemplate(templateName);
    const model = aiModel.getModel();
    const Parser = await getStringOutputParser();
    const chain = template.pipe(model).pipe(new Parser());

    logger.info('ğŸ¤– è°ƒç”¨ LLM', {
      provider: modelInfo.provider,
      model: modelInfo.model,
      template: templateName,
    });

    try {
      const response = await Promise.race([
        chain.invoke(variables),
        new Promise((_, reject) =>
          setTimeout(() => reject(new Error('LLM è°ƒç”¨è¶…æ—¶ï¼ˆ180ç§’ï¼‰')), 180000)
        ),
      ]);

      logger.info('âœ… LLM ç”Ÿæˆå®Œæˆ', {
        provider: modelInfo.provider,
        model: modelInfo.model,
        template: templateName,
      });

      return response;
    } catch (error) {
      logger.error('âŒ LLM è°ƒç”¨å¤±è´¥', {
        provider: modelInfo.provider,
        model: modelInfo.model,
        template: templateName,
        error: error.message,
      });
      throw error;
    }
  }

  /**
   * ä½¿ç”¨æ¨¡æ¿ç”Ÿæˆå†…å®¹ï¼ˆæµå¼ï¼‰
   * @param {string} templateName - æ¨¡æ¿åç§°
   * @param {Object} variables - æ¨¡æ¿å˜é‡
   * @param {Function} onChunk - chunk å›è°ƒ
   * @param {Object} options - é…ç½®é€‰é¡¹
   */
  async streamGenerate(templateName, variables, onChunk, options = {}) {
    this._ensureInitialized();

    const { taskId = `${templateName}_${Date.now()}` } = options;
    const modelInfo = aiModel.getCurrentModel();

    logger.info('ğŸ¤– è°ƒç”¨ LLM (æµå¼æ¨¡æ¿)', {
      provider: modelInfo.provider,
      model: modelInfo.model,
      type: 'stream_generate',
      template: templateName,
      taskId,
    });

    const template = promptManager.getTemplate(templateName);
    const streamingModel = aiModel.getStreamingModel(); // ä½¿ç”¨æµå¼æ¨¡å‹
    const Parser = await getStringOutputParser();
    const chain = template.pipe(streamingModel).pipe(new Parser());

    const stream = await chain.stream(variables);

    const controller = streamManager.createStream(taskId, stream, options);

    const result = await controller.start(onChunk);

    logger.info('âœ… LLM æµå¼æ¨¡æ¿ç”Ÿæˆå®Œæˆ', {
      provider: modelInfo.provider,
      model: modelInfo.model,
      template: templateName,
      taskId,
      contentLength: result.length,
    });

    return result;
  }

  /**
   * å–æ¶ˆæµå¼ä»»åŠ¡
   * @param {string} taskId - ä»»åŠ¡ID
   */
  cancelStream(taskId) {
    return streamManager.cancelStream(taskId);
  }

  /**
   * è·å–æµå¼ä»»åŠ¡çŠ¶æ€
   * @param {string} taskId - ä»»åŠ¡ID
   */
  getStreamStatus(taskId) {
    const controller = streamManager.getStream(taskId);
    return controller ? controller.getStatus() : null;
  }

  /**
   * è·å–æœåŠ¡ä¿¡æ¯
   */
  getInfo() {
    return {
      ...aiModel.getInfo(),
      activeStreams: streamManager.getActiveCount(),
      availableTemplates: promptManager.getAvailableTemplates(),
    };
  }

  /**
   * æ£€æŸ¥æœåŠ¡æ˜¯å¦å¯ç”¨
   */
  isAvailable() {
    return this.initialized && aiModel.isAvailable();
  }
}

module.exports = new AIService();
